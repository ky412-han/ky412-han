{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from langchain_teddynote.models import LLMs\n",
    "\n",
    "# 사용자 요구사항 수집을 위한 시스템 메시지 템플릿\n",
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\n",
    "\n",
    "[IMPORTANT] Your conversation should be in Korean. Your generated prompt should be in English.\"\"\"\n",
    "\n",
    "\n",
    "# 사용자 메시지 목록을 받아 시스템 메시지와 결합하여 반환\n",
    "def get_messages_info(messages):\n",
    "    # 사용자 요구사항 수집을 위한 시스템 메시지와 기존 메시지 결합\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "# LLM에 대한 프롬프트 지침을 정의하는 데이터 모델\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    # 프롬프트의 목표\n",
    "    objective: str\n",
    "    # 프롬프트 템플릿에 전달될 변수 목록\n",
    "    variables: List[str]\n",
    "    # 출력에서 피해야 할 제약 조건 목록\n",
    "    constraints: List[str]\n",
    "    # 출력이 반드시 따라야 할 요구 사항 목록\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(temperature=0, model=\"gtp-4o-mini\")\n",
    "# PromptInstructions 구조체를 바인딩\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "# 상태 정보를 기반으로 메시지 체인을 생성하고 LLM 호출\n",
    "def info_chain(state):\n",
    "    # 상태에서 메시지 정보를 가져와 시스템 메시지와 결합\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    # LLM을 호출하여 응답 생성\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    # 생성된 응답을 메시지 목록으로 반환\n",
    "    return {\"messages\": [response]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
